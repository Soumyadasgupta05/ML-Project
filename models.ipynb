{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dcc6aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Union\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import SchNet, DimeNetPlusPlus, global_add_pool, global_mean_pool\n",
    "import torch_scatter\n",
    "from torch_scatter import scatter\n",
    "from e3nn import o3\n",
    "#src\n",
    "from modules.blocks import (\n",
    "    EquivariantProductBasisBlock,\n",
    "    RadialEmbeddingBlock,\n",
    ")\n",
    "#src\n",
    "from modules.irreps_tools import reshape_irreps\n",
    "#src*3\n",
    "from egnn_layers import MPNNLayer, EGNNLayer\n",
    "from tfn_layers import TensorProductConvLayer\n",
    "import gvp_layers as gvp\n",
    "from comenet_layers import Linear, TwoLayerLinear, EmbeddingBlock, EdgeGraphConv, SimpleInteractionBlock\n",
    "\n",
    "###\n",
    "\n",
    "from torch_cluster import radius_graph\n",
    "from torch_geometric.nn import GraphConv, GraphNorm\n",
    "from torch_geometric.nn import inits\n",
    "\n",
    "from aloo import angle_emb, torsion_emb\n",
    "\n",
    "from torch_scatter import scatter, scatter_min\n",
    "\n",
    "from torch.nn import Embedding\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "try:\n",
    "    import sympy as sym\n",
    "except ImportError:\n",
    "    sym = None\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "###\n",
    "\n",
    "class MACEModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        r_max=10.0,\n",
    "        num_bessel=8,\n",
    "        num_polynomial_cutoff=5,\n",
    "        max_ell=2,\n",
    "        correlation=3,\n",
    "        num_layers=5,\n",
    "        emb_dim=64,\n",
    "        in_dim=1,\n",
    "        out_dim=1,\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True,\n",
    "        scalar_pred=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.r_max = r_max\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.residual = residual\n",
    "        self.scalar_pred = scalar_pred\n",
    "        # Embedding\n",
    "        self.radial_embedding = RadialEmbeddingBlock(\n",
    "            r_max=r_max,\n",
    "            num_bessel=num_bessel,\n",
    "            num_polynomial_cutoff=num_polynomial_cutoff,\n",
    "        )\n",
    "        sh_irreps = o3.Irreps.spherical_harmonics(max_ell)\n",
    "        self.spherical_harmonics = o3.SphericalHarmonics(\n",
    "            sh_irreps, normalize=True, normalization=\"component\"\n",
    "        )\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = torch.nn.Embedding(in_dim, emb_dim)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.prods = torch.nn.ModuleList()\n",
    "        self.reshapes = torch.nn.ModuleList()\n",
    "        hidden_irreps = (sh_irreps * emb_dim).sort()[0].simplify()\n",
    "        irrep_seq = [\n",
    "            o3.Irreps(f'{emb_dim}x0e'),\n",
    "            # o3.Irreps(f'{emb_dim}x0e + {emb_dim}x1o + {emb_dim}x2e'),\n",
    "            # o3.Irreps(f'{emb_dim//2}x0e + {emb_dim//2}x0o + {emb_dim//2}x1e + {emb_dim//2}x1o + {emb_dim//2}x2e + {emb_dim//2}x2o'),\n",
    "            hidden_irreps\n",
    "        ]\n",
    "        for i in range(num_layers):\n",
    "            in_irreps = irrep_seq[min(i, len(irrep_seq) - 1)]\n",
    "            out_irreps = irrep_seq[min(i + 1, len(irrep_seq) - 1)]\n",
    "            conv = TensorProductConvLayer(\n",
    "                in_irreps=in_irreps,\n",
    "                out_irreps=out_irreps,\n",
    "                sh_irreps=sh_irreps,\n",
    "                edge_feats_dim=self.radial_embedding.out_dim,\n",
    "                hidden_dim=emb_dim,\n",
    "                gate=False,\n",
    "                aggr=aggr,\n",
    "            )\n",
    "            self.convs.append(conv)\n",
    "            self.reshapes.append(reshape_irreps(out_irreps))\n",
    "            prod = EquivariantProductBasisBlock(\n",
    "                node_feats_irreps=out_irreps,\n",
    "                target_irreps=out_irreps,\n",
    "                correlation=correlation,\n",
    "                element_dependent=False,\n",
    "                num_elements=in_dim,\n",
    "                use_sc=residual\n",
    "            )\n",
    "            self.prods.append(prod)\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        if self.scalar_pred:\n",
    "            # Predictor MLP\n",
    "            self.pred = torch.nn.Sequential(\n",
    "                torch.nn.Linear(emb_dim, emb_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(emb_dim, out_dim)\n",
    "            )\n",
    "        else:\n",
    "            self.pred = torch.nn.Linear(hidden_irreps.dim, out_dim)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        h = self.emb_in(batch.atoms)  # (n,) -> (n, d)\n",
    "\n",
    "        # Edge features\n",
    "        vectors = batch.pos[batch.edge_index[0]] - batch.pos[batch.edge_index[1]]  # [n_edges, 3]\n",
    "        lengths = torch.linalg.norm(vectors, dim=-1, keepdim=True)  # [n_edges, 1]\n",
    "        edge_attrs = self.spherical_harmonics(vectors)\n",
    "        edge_feats = self.radial_embedding(lengths)\n",
    "        \n",
    "        for conv, reshape, prod in zip(self.convs, self.reshapes, self.prods):\n",
    "            # Message passing layer\n",
    "            h_update = conv(h, batch.edge_index, edge_attrs, edge_feats)\n",
    "            # Update node features\n",
    "            sc = F.pad(h, (0, h_update.shape[-1] - h.shape[-1]))\n",
    "            h = prod(reshape(h_update), sc, None)\n",
    "\n",
    "        if self.scalar_pred:\n",
    "            # Select only scalars for prediction\n",
    "            h = h[:,:self.emb_dim]\n",
    "        out = self.pool(h, batch.batch)  # (n, d) -> (batch_size, d)\n",
    "        return self.pred(out)  # (batch_size, out_dim)\n",
    "\n",
    "\n",
    "class TFNModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        r_max=10.0,\n",
    "        num_bessel=8,\n",
    "        num_polynomial_cutoff=5,\n",
    "        max_ell=2,\n",
    "        num_layers=5,\n",
    "        emb_dim=64,\n",
    "        in_dim=1,\n",
    "        out_dim=1,\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True,\n",
    "        scalar_pred=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.r_max = r_max\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.residual = residual\n",
    "        self.scalar_pred = scalar_pred\n",
    "        # Embedding\n",
    "        self.radial_embedding = RadialEmbeddingBlock(\n",
    "            r_max=r_max,\n",
    "            num_bessel=num_bessel,\n",
    "            num_polynomial_cutoff=num_polynomial_cutoff,\n",
    "        )\n",
    "        sh_irreps = o3.Irreps.spherical_harmonics(max_ell)\n",
    "        self.spherical_harmonics = o3.SphericalHarmonics(\n",
    "            sh_irreps, normalize=True, normalization=\"component\"\n",
    "        )\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = torch.nn.Embedding(in_dim, emb_dim)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        hidden_irreps = (sh_irreps * emb_dim).sort()[0].simplify()\n",
    "        irrep_seq = [\n",
    "            o3.Irreps(f'{emb_dim}x0e'),\n",
    "            # o3.Irreps(f'{emb_dim}x0e + {emb_dim}x1o + {emb_dim}x2e'),\n",
    "            # o3.Irreps(f'{emb_dim//2}x0e + {emb_dim//2}x0o + {emb_dim//2}x1e + {emb_dim//2}x1o + {emb_dim//2}x2e + {emb_dim//2}x2o'),\n",
    "            hidden_irreps\n",
    "        ]\n",
    "        for i in range(num_layers):\n",
    "            in_irreps = irrep_seq[min(i, len(irrep_seq) - 1)]\n",
    "            out_irreps = irrep_seq[min(i + 1, len(irrep_seq) - 1)]\n",
    "            conv = TensorProductConvLayer(\n",
    "                in_irreps=in_irreps,\n",
    "                out_irreps=out_irreps,\n",
    "                sh_irreps=sh_irreps,\n",
    "                edge_feats_dim=self.radial_embedding.out_dim,\n",
    "                hidden_dim=emb_dim,\n",
    "                gate=True,\n",
    "                aggr=aggr,\n",
    "            )\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        if self.scalar_pred:\n",
    "            # Predictor MLP\n",
    "            self.pred = torch.nn.Sequential(\n",
    "                torch.nn.Linear(emb_dim, emb_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(emb_dim, out_dim)\n",
    "            )\n",
    "        else:\n",
    "            self.pred = torch.nn.Linear(hidden_irreps.dim, out_dim)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        h = self.emb_in(batch.atoms)  # (n,) -> (n, d)\n",
    "\n",
    "        # Edge features\n",
    "        vectors = batch.pos[batch.edge_index[0]] - batch.pos[batch.edge_index[1]]  # [n_edges, 3]\n",
    "        lengths = torch.linalg.norm(vectors, dim=-1, keepdim=True)  # [n_edges, 1]\n",
    "        edge_attrs = self.spherical_harmonics(vectors)\n",
    "        edge_feats = self.radial_embedding(lengths)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update = conv(h, batch.edge_index, edge_attrs, edge_feats)\n",
    "\n",
    "            # Update node features\n",
    "            h = h_update + F.pad(h, (0, h_update.shape[-1] - h.shape[-1])) if self.residual else h_update\n",
    "\n",
    "        if self.scalar_pred:\n",
    "            # Select only scalars for prediction\n",
    "            h = h[:,:self.emb_dim]\n",
    "        out = self.pool(h, batch.batch)  # (n, d) -> (batch_size, d)\n",
    "        return self.pred(out)  # (batch_size, out_dim)\n",
    "\n",
    "\n",
    "class GVPGNNModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        r_max=10.0,\n",
    "        num_bessel=8,\n",
    "        num_polynomial_cutoff=5,\n",
    "        num_layers=5,\n",
    "        emb_dim=64,\n",
    "        in_dim=1,\n",
    "        out_dim=1,\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        _DEFAULT_V_DIM = (emb_dim, emb_dim)\n",
    "        _DEFAULT_E_DIM = (emb_dim, 1)\n",
    "        activations = (F.relu, None)\n",
    "\n",
    "        self.r_max = r_max\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_layers = num_layers\n",
    "        # Embedding\n",
    "        self.radial_embedding = RadialEmbeddingBlock(\n",
    "            r_max=r_max,\n",
    "            num_bessel=num_bessel,\n",
    "            num_polynomial_cutoff=num_polynomial_cutoff,\n",
    "        )\n",
    "        self.emb_in = torch.nn.Embedding(in_dim, emb_dim)\n",
    "        self.W_e = torch.nn.Sequential(\n",
    "            gvp.LayerNorm((self.radial_embedding.out_dim, 1)),\n",
    "            gvp.GVP((self.radial_embedding.out_dim, 1), _DEFAULT_E_DIM, \n",
    "                activations=(None, None), vector_gate=True)\n",
    "        )\n",
    "        self.W_v = torch.nn.Sequential(\n",
    "            gvp.LayerNorm((emb_dim, 0)),\n",
    "            gvp.GVP((emb_dim, 0), _DEFAULT_V_DIM,\n",
    "                activations=(None, None), vector_gate=True)\n",
    "        )\n",
    "        \n",
    "        # Stack of GNN layers\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "                gvp.GVPConvLayer(_DEFAULT_V_DIM, _DEFAULT_E_DIM, \n",
    "                             activations=activations, vector_gate=True,\n",
    "                             residual=residual) \n",
    "            for _ in range(num_layers))\n",
    "        \n",
    "        self.W_out = torch.nn.Sequential(\n",
    "            gvp.LayerNorm(_DEFAULT_V_DIM),\n",
    "            gvp.GVP(_DEFAULT_V_DIM, (emb_dim, 0), \n",
    "                activations=activations, vector_gate=True)\n",
    "        )\n",
    "        \n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        # Predictor MLP\n",
    "        self.pred = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_dim, emb_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_dim, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, batch):\n",
    "\n",
    "        # Edge features\n",
    "        vectors = batch.pos[batch.edge_index[0]] - batch.pos[batch.edge_index[1]]  # [n_edges, 3]\n",
    "        lengths = torch.linalg.norm(vectors, dim=-1, keepdim=True)  # [n_edges, 1]\n",
    "        \n",
    "        h_V = self.emb_in(batch.atoms)  # (n,) -> (n, d)\n",
    "        h_E = (self.radial_embedding(lengths), torch.nan_to_num(torch.div(vectors, lengths)).unsqueeze_(-2))\n",
    "\n",
    "        h_V = self.W_v(h_V)\n",
    "        h_E = self.W_e(h_E)\n",
    "    \n",
    "        for layer in self.layers:\n",
    "            h_V = layer(h_V, batch.edge_index, h_E)\n",
    "\n",
    "        out = self.W_out(h_V)\n",
    "        \n",
    "        out = self.pool(out, batch.batch)  # (n, d) -> (batch_size, d)\n",
    "        return self.pred(out)  # (batch_size, out_dim)\n",
    "\n",
    "\n",
    "class EGNNModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=5,\n",
    "        emb_dim=128,\n",
    "        in_dim=1,\n",
    "        out_dim=1,\n",
    "        activation=\"relu\",\n",
    "        norm=\"layer\",\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True\n",
    "    ):\n",
    "        \"\"\"E(n) Equivariant GNN model \n",
    "        \n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers\n",
    "            emb_dim: (int) - hidden dimension\n",
    "            in_dim: (int) - initial node feature dimension\n",
    "            out_dim: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = torch.nn.Embedding(in_dim, emb_dim)\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(EGNNLayer(emb_dim, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        # Predictor MLP\n",
    "        self.pred = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_dim, emb_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_dim, out_dim)\n",
    "        )\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        h = self.emb_in(batch.atoms)  # (n,) -> (n, d)\n",
    "        pos = batch.pos  # (n, 3)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update, pos_update = conv(h, pos, batch.edge_index)\n",
    "\n",
    "            # Update node features (n, d) -> (n, d)\n",
    "            h = h + h_update if self.residual else h_update \n",
    "\n",
    "            # Update node coordinates (no residual) (n, 3) -> (n, 3)\n",
    "            pos = pos_update\n",
    "\n",
    "        out = self.pool(h, batch.batch)  # (n, d) -> (batch_size, d)\n",
    "        return self.pred(out)  # (batch_size, out_dim)\n",
    "\n",
    "\n",
    "class MPNNModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=5,\n",
    "        emb_dim=128,\n",
    "        in_dim=1,\n",
    "        out_dim=1,\n",
    "        activation=\"relu\",\n",
    "        norm=\"layer\",\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True\n",
    "    ):\n",
    "        \"\"\"Vanilla Message Passing GNN model\n",
    "        \n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers\n",
    "            emb_dim: (int) - hidden dimension\n",
    "            in_dim: (int) - initial node feature dimension\n",
    "            out_dim: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = torch.nn.Embedding(in_dim, emb_dim)\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(MPNNLayer(emb_dim, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        # Predictor MLP\n",
    "        self.pred = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_dim, emb_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_dim, out_dim)\n",
    "        )\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        h = self.emb_in(batch.atoms)  # (n,) -> (n, d)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            # Message passing layer and residual connection\n",
    "            h = h + conv(h, batch.edge_index) if self.residual else conv(h, batch.edge_index)\n",
    "\n",
    "        out = self.pool(h, batch.batch)  # (n, d) -> (batch_size, d)\n",
    "        return self.pred(out)  # (batch_size, out_dim)\n",
    "\n",
    "\n",
    "class SchNetModel(SchNet):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_channels: int = 128, \n",
    "        in_dim: int = 1,\n",
    "        out_dim: int = 1, \n",
    "        num_filters: int = 128, \n",
    "        num_layers: int = 6,\n",
    "        num_gaussians: int = 50, \n",
    "        cutoff: float = 10, \n",
    "        max_num_neighbors: int = 32, \n",
    "        readout: str = 'add', \n",
    "        dipole: bool = False,\n",
    "        mean: Optional[float] = None, \n",
    "        std: Optional[float] = None, \n",
    "        atomref: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        super().__init__(hidden_channels, num_filters, num_layers, num_gaussians, cutoff, max_num_neighbors, readout, dipole, mean, std, atomref)\n",
    "\n",
    "        # Overwrite atom embedding and final predictor\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels // 2, out_dim)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        h = self.embedding(batch.atoms)\n",
    "\n",
    "        row, col = batch.edge_index\n",
    "        edge_weight = (batch.pos[row] - batch.pos[col]).norm(dim=-1)\n",
    "        edge_attr = self.distance_expansion(edge_weight)\n",
    "\n",
    "        for interaction in self.interactions:\n",
    "            h = h + interaction(h, batch.edge_index, edge_weight, edge_attr)\n",
    "\n",
    "        h = self.lin1(h)\n",
    "        h = self.act(h)\n",
    "        h = self.lin2(h)\n",
    "\n",
    "        out = scatter(h, batch.batch, dim=0, reduce=self.readout)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DimeNetPPModel(DimeNetPlusPlus):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_channels: int = 128, \n",
    "        in_dim: int = 1,\n",
    "        out_dim: int = 1, \n",
    "        num_layers: int = 4, \n",
    "        int_emb_size: int = 64, \n",
    "        basis_emb_size: int = 8, \n",
    "        out_emb_channels: int = 256, \n",
    "        num_spherical: int = 7, \n",
    "        num_radial: int = 6, \n",
    "        cutoff: float = 10, \n",
    "        max_num_neighbors: int = 32, \n",
    "        envelope_exponent: int = 5, \n",
    "        num_before_skip: int = 1, \n",
    "        num_after_skip: int = 2, \n",
    "        num_output_layers: int = 3, \n",
    "        act: Union[str, Callable] = 'swish'\n",
    "    ):\n",
    "        super().__init__(hidden_channels, out_dim, num_layers, int_emb_size, basis_emb_size, out_emb_channels, num_spherical, num_radial, cutoff, max_num_neighbors, envelope_exponent, num_before_skip, num_after_skip, num_output_layers, act)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        i, j, idx_i, idx_j, idx_k, idx_kj, idx_ji = self.triplets(\n",
    "            batch.edge_index, num_nodes=batch.atoms.size(0))\n",
    "\n",
    "        # Calculate distances.\n",
    "        dist = (batch.pos[i] - batch.pos[j]).pow(2).sum(dim=-1).sqrt()\n",
    "\n",
    "        # Calculate angles.\n",
    "        pos_i = batch.pos[idx_i]\n",
    "        pos_ji, pos_ki = batch.pos[idx_j] - pos_i, batch.pos[idx_k] - pos_i\n",
    "        a = (pos_ji * pos_ki).sum(dim=-1)\n",
    "        b = torch.cross(pos_ji, pos_ki).norm(dim=-1)\n",
    "        angle = torch.atan2(b, a)\n",
    "\n",
    "        rbf = self.rbf(dist)\n",
    "        sbf = self.sbf(dist, angle, idx_kj)\n",
    "\n",
    "        # Embedding block.\n",
    "        x = self.emb(batch.atoms, rbf, i, j)\n",
    "        P = self.output_blocks[0](x, rbf, i, num_nodes=batch.pos.size(0))\n",
    "\n",
    "        # Interaction blocks.\n",
    "        for interaction_block, output_block in zip(self.interaction_blocks,\n",
    "                                                   self.output_blocks[1:]):\n",
    "            x = interaction_block(x, rbf, sbf, idx_kj, idx_ji)\n",
    "            P += output_block(x, rbf, i)\n",
    "\n",
    "        return P.sum(dim=0) if batch is None else scatter(P, batch.batch, dim=0)\n",
    "\n",
    "    \n",
    "class ComENet(nn.Module):\n",
    "    r\"\"\"\n",
    "         The ComENet from the `\"ComENet: Towards Complete and Efficient Message Passing for 3D Molecular Graphs\" <https://arxiv.org/abs/2206.08515>`_ paper.\n",
    "        \n",
    "        Args:\n",
    "            cutoff (float, optional): Cutoff distance for interatomic interactions. (default: :obj:`8.0`)\n",
    "            num_layers (int, optional): Number of building blocks. (default: :obj:`4`)\n",
    "            hidden_channels (int, optional): Hidden embedding size. (default: :obj:`256`)\n",
    "            middle_channels (int, optional): Middle embedding size for the two layer linear block. (default: :obj:`256`)\n",
    "            out_channels (int, optional): Size of each output sample. (default: :obj:`1`)\n",
    "            num_radial (int, optional): Number of radial basis functions. (default: :obj:`3`)\n",
    "            num_spherical (int, optional): Number of spherical harmonics. (default: :obj:`2`)\n",
    "            num_output_layers (int, optional): Number of linear layers for the output blocks. (default: :obj:`3`)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            cutoff=8.0,\n",
    "            num_layers=4,\n",
    "            hidden_channels=256,\n",
    "            middle_channels=64,\n",
    "            out_channels=1,\n",
    "            num_radial=3,\n",
    "            num_spherical=2,\n",
    "            num_output_layers=3,\n",
    "    ):\n",
    "        super(ComENet, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.cutoff = cutoff\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        if sym is None:\n",
    "            raise ImportError(\"Package `sympy` could not be found.\")\n",
    "\n",
    "        act = swish\n",
    "        self.act = act\n",
    "\n",
    "        self.feature1 = torsion_emb(num_radial=num_radial, num_spherical=num_spherical, cutoff=cutoff)\n",
    "        self.feature2 = angle_emb(num_radial=num_radial, num_spherical=num_spherical, cutoff=cutoff)\n",
    "\n",
    "        self.emb = EmbeddingBlock(hidden_channels, act)\n",
    "\n",
    "        self.interaction_blocks = torch.nn.ModuleList(\n",
    "            [\n",
    "                SimpleInteractionBlock(\n",
    "                    hidden_channels,\n",
    "                    middle_channels,\n",
    "                    num_radial,\n",
    "                    num_spherical,\n",
    "                    num_output_layers,\n",
    "                    hidden_channels,\n",
    "                    act,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        for _ in range(num_output_layers):\n",
    "            self.lins.append(Linear(hidden_channels, hidden_channels))\n",
    "        self.lin_out = Linear(hidden_channels, out_channels)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.emb.reset_parameters()\n",
    "        for interaction in self.interaction_blocks:\n",
    "            interaction.reset_parameters()\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        self.lin_out.reset_parameters()\n",
    "\n",
    "    def _forward(self, data):\n",
    "        batch = data.batch\n",
    "        z = data.atoms.long()\n",
    "        pos = data.pos\n",
    "        num_nodes = z.size(0)\n",
    "\n",
    "        %edge_index = radius_graph(pos, r=self.cutoff, batch=batch)\n",
             edge_index = data.edge_index
    "        j, i = edge_index\n",
    "\n",
    "        vecs = pos[j] - pos[i]\n",
    "        dist = vecs.norm(dim=-1)\n",
    "\n",
    "        # Embedding block.\n",
    "        x = self.emb(z)\n",
    "\n",
    "        # Calculate distances.\n",
    "        _, argmin0 = scatter_min(dist, i, dim_size=num_nodes)\n",
    "        argmin0[argmin0 >= len(i)] = 0\n",
    "        n0 = j[argmin0]\n",
    "        add = torch.zeros_like(dist).to(dist.device)\n",
    "        add[argmin0] = self.cutoff\n",
    "        dist1 = dist + add\n",
    "\n",
    "        _, argmin1 = scatter_min(dist1, i, dim_size=num_nodes)\n",
    "        argmin1[argmin1 >= len(i)] = 0\n",
    "        n1 = j[argmin1]\n",
    "        # --------------------------------------------------------\n",
    "\n",
    "        _, argmin0_j = scatter_min(dist, j, dim_size=num_nodes)\n",
    "        argmin0_j[argmin0_j >= len(j)] = 0\n",
    "        n0_j = i[argmin0_j]\n",
    "\n",
    "        add_j = torch.zeros_like(dist).to(dist.device)\n",
    "        add_j[argmin0_j] = self.cutoff\n",
    "        dist1_j = dist + add_j\n",
    "\n",
    "        # i[argmin] = range(0, num_nodes)\n",
    "        _, argmin1_j = scatter_min(dist1_j, j, dim_size=num_nodes)\n",
    "        argmin1_j[argmin1_j >= len(j)] = 0\n",
    "        n1_j = i[argmin1_j]\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "\n",
    "        # n0, n1 for i\n",
    "        n0 = n0[i]\n",
    "        n1 = n1[i]\n",
    "\n",
    "        # n0, n1 for j\n",
    "        n0_j = n0_j[j]\n",
    "        n1_j = n1_j[j]\n",
    "\n",
    "        # tau: (iref, i, j, jref)\n",
    "        # when compute tau, do not use n0, n0_j as ref for i and j,\n",
    "        # because if n0 = j, or n0_j = i, the computed tau is zero\n",
    "        # so if n0 = j, we choose iref = n1\n",
    "        # if n0_j = i, we choose jref = n1_j\n",
    "        mask_iref = n0 == j\n",
    "        iref = torch.clone(n0)\n",
    "        iref[mask_iref] = n1[mask_iref]\n",
    "        idx_iref = argmin0[i]\n",
    "        idx_iref[mask_iref] = argmin1[i][mask_iref]\n",
    "\n",
    "        mask_jref = n0_j == i\n",
    "        jref = torch.clone(n0_j)\n",
    "        jref[mask_jref] = n1_j[mask_jref]\n",
    "        idx_jref = argmin0_j[j]\n",
    "        idx_jref[mask_jref] = argmin1_j[j][mask_jref]\n",
    "\n",
    "        pos_ji, pos_in0, pos_in1, pos_iref, pos_jref_j = (\n",
    "            vecs,\n",
    "            vecs[argmin0][i],\n",
    "            vecs[argmin1][i],\n",
    "            vecs[idx_iref],\n",
    "            vecs[idx_jref]\n",
    "        )\n",
    "\n",
    "        # Calculate angles.\n",
    "        a = ((-pos_ji) * pos_in0).sum(dim=-1)\n",
    "        b = torch.cross(-pos_ji, pos_in0).norm(dim=-1)\n",
    "        theta = torch.atan2(b, a)\n",
    "        theta[theta < 0] = theta[theta < 0] + math.pi\n",
    "\n",
    "        # Calculate torsions.\n",
    "        dist_ji = pos_ji.pow(2).sum(dim=-1).sqrt()\n",
    "        plane1 = torch.cross(-pos_ji, pos_in0)\n",
    "        plane2 = torch.cross(-pos_ji, pos_in1)\n",
    "        a = (plane1 * plane2).sum(dim=-1)  # cos_angle * |plane1| * |plane2|\n",
    "        b = (torch.cross(plane1, plane2) * pos_ji).sum(dim=-1) / dist_ji\n",
    "        phi = torch.atan2(b, a)\n",
    "        phi[phi < 0] = phi[phi < 0] + math.pi\n",
    "\n",
    "        # Calculate right torsions.\n",
    "        plane1 = torch.cross(pos_ji, pos_jref_j)\n",
    "        plane2 = torch.cross(pos_ji, pos_iref)\n",
    "        a = (plane1 * plane2).sum(dim=-1)  # cos_angle * |plane1| * |plane2|\n",
    "        b = (torch.cross(plane1, plane2) * pos_ji).sum(dim=-1) / dist_ji\n",
    "        tau = torch.atan2(b, a)\n",
    "        tau[tau < 0] = tau[tau < 0] + math.pi\n",
    "\n",
    "        feature1 = self.feature1(dist, theta, phi)\n",
    "        feature2 = self.feature2(dist, tau)\n",
    "\n",
    "        # Interaction blocks.\n",
    "        for interaction_block in self.interaction_blocks:\n",
    "            x = interaction_block(x, feature1, feature2, edge_index, batch)\n",
    "\n",
    "        for lin in self.lins:\n",
    "            x = self.act(lin(x))\n",
    "        x = self.lin_out(x)\n",
    "\n",
    "        energy = scatter(x, batch, dim=0)\n",
    "        return energy\n",
    "\n",
    "    def forward(self, batch_data):\n",
    "        return self._forward(batch_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
